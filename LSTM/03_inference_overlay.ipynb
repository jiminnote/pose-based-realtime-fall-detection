{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68ff5d7",
   "metadata": {},
   "source": [
    "- 입력: mp4 (또는 webcam)\n",
    "- 처리: YOLOv8-pose → (frame feature 85) → 최근 T=40 프레임 LSTM 추론\n",
    "- 출력: mp4 (확률 오버레이 포함)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b42604",
   "metadata": {},
   "source": [
    "## 1) Imports & 경로/파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "773bac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "626cb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로 =====\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "MODEL_PATH = Path(\"./best_lstm.pt\")          # 02_train_lstm.ipynb에서 저장한 파일\n",
    "YOLO_POSE_WEIGHTS = \"yolov8n-pose.pt\"        # 또는 본인 가중치 경로\n",
    "\n",
    "# 입력/출력 영상\n",
    "INPUT_VIDEO = str(DATA_ROOT / \"train\" / \"raw\" /\"video\" / \"N\" / \"N\" / \"00010_H_A_N_C4\" / \"00010_H_A_N_C4.mp4\")\n",
    "OUTPUT_VIDEO = \"out/00010_H_A_N_C4_overlay.mp4\"\n",
    "\n",
    "# ===== 학습과 동일해야 하는 파라미터 =====\n",
    "NUM_FRAMES = 100          # (학습에서 고정)\n",
    "T = 40                    # sequence length (4초 @10fps)\n",
    "INPUT_DIM = 85            # feature dim\n",
    "FALL_CLASS_INDEX = 1      # softmax에서 낙상 클래스 인덱스\n",
    "\n",
    "# ===== 추론 설정 =====\n",
    "TARGET_FPS = 10           # 학습 fps에 맞추기 위해 프레임 샘플링 (입력 fps가 달라도 10fps로 처리)\n",
    "EMA_ALPHA = 0.2           # 확률 스무딩 (0~1) - 클수록 즉시 반응\n",
    "SHOW_SKELETON = True      # keypoint 표시 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec2832",
   "metadata": {},
   "source": [
    "## 2) LSTM 모델 정의 & 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af387b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a4eb3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=85, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7dc7a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: best_lstm.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/f97855ms55x_bvptq4cmnmmh0000gn/T/ipykernel_73914/103494679.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "model = FallLSTM(input_dim=INPUT_DIM).to(DEVICE)\n",
    "state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "print(\"loaded:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e00087",
   "metadata": {},
   "source": [
    "## 3) YOLOv8 Pose 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b8f247b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded yolo: yolov8n-pose.pt\n"
     ]
    }
   ],
   "source": [
    "pose_model = YOLO(YOLO_POSE_WEIGHTS)\n",
    "print(\"loaded yolo:\", YOLO_POSE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf3c49",
   "metadata": {},
   "source": [
    "## 4) 학습과 동일한 전처리: frame → feature(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f56acd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpts_to_feature(xy: np.ndarray, conf: np.ndarray, prev_xy_norm: np.ndarray):\n",
    "    \"\"\"\n",
    "    xy: (17,2) raw pixel\n",
    "    conf: (17,)\n",
    "    prev_xy_norm: (17,2) or None  (이전 프레임 정규화 xy)\n",
    "\n",
    "    return:\n",
    "      feat: (85,)\n",
    "      xy_norm: (17,2)  (다음 프레임 velocity 계산용)\n",
    "    \"\"\"\n",
    "    xy = xy.astype(np.float32)\n",
    "    conf = conf.astype(np.float32)\n",
    "\n",
    "    # hip center (11,12)\n",
    "    center = (xy[11] + xy[12]) / 2.0\n",
    "    xy_c = xy - center\n",
    "\n",
    "    # scale (shoulder-hip) with epsilon\n",
    "    scale = np.linalg.norm(xy_c[5] - xy_c[11]) + 1e-6\n",
    "    xy_n = xy_c / scale\n",
    "\n",
    "    if prev_xy_norm is None:\n",
    "        vel = np.zeros_like(xy_n, dtype=np.float32)\n",
    "    else:\n",
    "        vel = (xy_n - prev_xy_norm).astype(np.float32)\n",
    "\n",
    "    feat = np.concatenate([xy_n.flatten(), conf, vel.flatten()]).astype(np.float32)  # 34+17+34=85\n",
    "    return feat, xy_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280c2f0",
   "metadata": {},
   "source": [
    "## 5) YOLO 포즈 추출: frame → (xy, conf) or zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b306d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_one(frame_bgr: np.ndarray):\n",
    "    \"\"\"\n",
    "    return:\n",
    "      xy: (17,2)\n",
    "      conf: (17,)\n",
    "      has_person: bool\n",
    "    \"\"\"\n",
    "    res = pose_model(frame_bgr, verbose=False)[0]\n",
    "\n",
    "    if (res.keypoints is None) or (res.keypoints.xy is None) or (len(res.keypoints.xy) == 0):\n",
    "        xy = np.zeros((17,2), dtype=np.float32)\n",
    "        conf = np.zeros((17,), dtype=np.float32)\n",
    "        return xy, conf, False\n",
    "\n",
    "    # 첫 번째 사람만 사용 (필요 시: 가장 큰 bbox 기준 등으로 개선 가능)\n",
    "    xy = res.keypoints.xy[0].detach().cpu().numpy().astype(np.float32)      # (17,2)\n",
    "    conf = res.keypoints.conf[0].detach().cpu().numpy().astype(np.float32)  # (17,)\n",
    "    return xy, conf, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d22d",
   "metadata": {},
   "source": [
    "## 6) 오버레이 유틸 (텍스트 + 바)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "52481577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_overlay(frame, p_fall, p_norm, x=20, y=40):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, f\"Fall: {p_fall:.2f}\", (x, y), font, 0.9, (0,0,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f\"Normal: {p_norm:.2f}\", (x, y+35), font, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # bar\n",
    "    bar_w, bar_h = 240, 18\n",
    "    bx, by = x, y+60\n",
    "    cv2.rectangle(frame, (bx, by), (bx+bar_w, by+bar_h), (255,255,255), 2)\n",
    "    fill = int(bar_w * float(p_fall))\n",
    "    cv2.rectangle(frame, (bx, by), (bx+fill, by+bar_h), (0,0,255), -1)\n",
    "    return frame\n",
    "\n",
    "def draw_keypoints(frame, xy, conf, thr=0.3):\n",
    "    for i in range(17):\n",
    "        if conf[i] >= thr:\n",
    "            cx, cy = int(xy[i,0]), int(xy[i,1])\n",
    "            cv2.circle(frame, (cx,cy), 3, (255,255,0), -1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b348fea",
   "metadata": {},
   "source": [
    "## 7) 비디오 추론 + 저장 (mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b04ed6",
   "metadata": {},
   "source": [
    "- 입력 영상 fps가 10이 아니면 TARGET_FPS=10에 맞춰 샘플링해서 처리합니다.\n",
    "- LSTM은 최근 T 프레임 feature가 쌓여야 추론을 시작합니다.\n",
    "- 확률은 EMA로 스무딩해서 흔들림을 줄입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6c2dcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fps: 10.0 size: (1280, 720)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"cannot open: {INPUT_VIDEO}\")\n",
    "\n",
    "in_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"input fps:\", in_fps, \"size:\", (w,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5243b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: out/00010_H_A_N_C4_overlay.mp4 frames: 100\n"
     ]
    }
   ],
   "source": [
    "# output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, float(in_fps if in_fps > 1 else TARGET_FPS), (w, h))\n",
    "\n",
    "# streaming buffers\n",
    "feat_buf = deque(maxlen=T)\n",
    "prev_xy_norm = None\n",
    "p_fall_ema = 0.0\n",
    "\n",
    "# fps sampling\n",
    "if in_fps and in_fps > 1:\n",
    "    step = max(1, int(round(in_fps / TARGET_FPS)))\n",
    "else:\n",
    "    step = 1\n",
    "\n",
    "frame_idx = 0\n",
    "written = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "        # 샘플링: TARGET_FPS에 맞춰 처리\n",
    "        if frame_idx % step != 0:\n",
    "            out.write(frame)\n",
    "            written += 1\n",
    "            continue\n",
    "\n",
    "        xy, conf, has_person = extract_pose_one(frame)\n",
    "\n",
    "        if SHOW_SKELETON:\n",
    "            frame = draw_keypoints(frame, xy, conf, thr=0.3)\n",
    "\n",
    "        # feature 생성\n",
    "        feat, prev_xy_norm = kpts_to_feature(xy, conf, prev_xy_norm)\n",
    "        feat_buf.append(feat)\n",
    "\n",
    "        # 기본값 (버퍼가 차기 전)\n",
    "        p_fall = p_fall_ema\n",
    "        p_norm = 1.0 - p_fall_ema\n",
    "\n",
    "        # 추론 (T개 쌓이면)\n",
    "        if len(feat_buf) == T:\n",
    "            x_seq = np.stack(list(feat_buf), axis=0)[None, :, :]  # (1,T,85)\n",
    "            x_t = torch.tensor(x_seq, dtype=torch.float32, device=DEVICE)\n",
    "            logits = model(x_t)\n",
    "            prob = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "            p_fall_raw = float(prob[FALL_CLASS_INDEX])\n",
    "            # EMA smoothing\n",
    "            p_fall_ema = EMA_ALPHA * p_fall_raw + (1 - EMA_ALPHA) * p_fall_ema\n",
    "\n",
    "            p_fall = p_fall_ema\n",
    "            p_norm = 1.0 - p_fall_ema\n",
    "\n",
    "        # 오버레이\n",
    "        frame = draw_overlay(frame, p_fall, p_norm)\n",
    "\n",
    "        out.write(frame)\n",
    "        written += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"saved:\", OUTPUT_VIDEO, \"frames:\", written)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
